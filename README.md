# ğŸ” LLM Response Reliability Evaluation Pipeline

A lightweight, production-oriented evaluation pipeline to automatically assess the **reliability of LLM-generated responses** in real-time.

This project was built as part of the **BeyondChats Internship Assignment** and is designed with scalability, interpretability, and cost-efficiency in mind.

---

## ğŸš€ Why This Project?

Large Language Models (LLMs) can generate fluent responses, but fluency alone does not guarantee:
- relevance to the user query,
- completeness of the answer,
- or factual correctness (hallucinations).

In real-world products, especially in **high-risk domains like healthcare**, it is critical to **systematically evaluate LLM outputs** before trusting them.

This pipeline addresses exactly that problem.

---

## ğŸ“Œ What This Pipeline Evaluates

For every **User â†’ AI response pair**, the pipeline computes:

### 1ï¸âƒ£ Response Relevance  
Measures how semantically aligned the AI response is with the user query.

### 2ï¸âƒ£ Response Completeness  
Checks whether the response sufficiently covers the key aspects of the userâ€™s question.

### 3ï¸âƒ£ Hallucination / Factual Accuracy  
Identifies unsupported or hallucinated claims by verifying each response sentence against retrieved context.

### 4ï¸âƒ£ Latency  
Estimates response generation latency using message timestamps.

### 5ï¸âƒ£ Cost  
Estimates token-based inference cost using configurable pricing.

---

## ğŸ§  High-Level Architecture

```bash
Chat JSON â”€â”€â”
â”œâ”€â–¶ Evaluation Pipeline â”€â”€â–¶ evaluation_report.json
Context JSON â”€â”˜

```

### **Key Design Choice:**  
The pipeline evaluates **only the context vectors actually used by the RAG system**, ensuring fair and accurate hallucination detection.

---


## ğŸ—ï¸ Repository Structure

```
llm-evaluation-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚ â””â”€â”€ evaluator.py # Core evaluation logic
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ chat1.json # Chat conversation (sample 1)
â”‚ â”œâ”€â”€ context1.json # Vector DB context (sample 1)
â”‚ â”œâ”€â”€ chat2.json # Chat conversation (sample 2)
â”‚ â”œâ”€â”€ context2.json # Vector DB context (sample 2)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```

---


## âš™ï¸ Local Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/llm-evaluation-pipeline.git
cd llm-evaluation-pipeline

```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt

```
###3ï¸âƒ£ Run the Evaluation

```bash
python src/evaluator.py

```

ğŸ“„ Output will be generated as:

```bash
evaluation_report.json

```

---


## ğŸ“Š Evaluation Methodology

### ğŸ”¹ Relevance
Uses a lightweight CrossEncoder model

Produces a normalized score âˆˆ [0, 1]

### ğŸ”¹ Completeness
Keyword coverage heuristic

Optimized for speed and interpretability

Suitable for large-scale real-time evaluation

### ğŸ”¹ Hallucination Detection
Sentence-level verification

Each sentence is checked for entailment against retrieved context

Implemented using a Natural Language Inference (NLI) model

âš ï¸ The hallucination metric is intentionally conservative to avoid false negatives in sensitive domains.

### ğŸ”¹ Latency & Cost
Latency derived from timestamps

Cost estimated using token counts and configurable per-token pricing

---


## ğŸ“ˆ Example Output

```
{
  "dataset_id": 1,
  "turn_id": 14,
  "metrics": {
    "relevance": 0.92,
    "completeness": 0.66,
    "faithfulness": 0.0,
    "latency_sec": 9.0,
    "cost_usd": 0.000043
  },
  "hallucinated_sentences": [
    "We also offer specially subsidized rooms at our clinic."
  ]
}

```

---

## âš–ï¸ Design Decisions & Trade-offs

### Why not use an LLM to evaluate another LLM?
High latency

High operational cost

Circular dependency

### Why sentence-level hallucination detection?
Identifies exact unsupported claims

More actionable for debugging and monitoring

Commonly used in production trust & safety systems

### Why heuristic completeness instead of generative scoring?
Faster

Deterministic

Scales to millions of evaluations per day

---

## ğŸš€ Scalability & Production Readiness
This pipeline is designed to scale efficiently:

âŒ No external API calls

âœ… Lightweight models suitable for batch inference

âœ… Stateless evaluation â†’ easy horizontal scaling

âœ… Deterministic metrics for monitoring dashboards

---

##ğŸ“ Notes
Low faithfulness scores do not necessarily indicate poor responses â€” they indicate missing or weak grounding in retrieved context.

This conservative behavior is intentional and desirable for safety-critical applications.

---

##ğŸ‘¤ Author

Ishika Dubey
Applied for Part-Time Internship at BeyondChats
